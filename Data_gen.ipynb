{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Data_gen.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMC76YaCyfndD1J2M/ok27T"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"eJS8Y4y77Lf4","colab_type":"code","outputId":"0e5bcdd4-616c-4415-c89a-0d21235178ad","executionInfo":{"status":"ok","timestamp":1591278452340,"user_tz":240,"elapsed":23828,"user":{"displayName":"Jongyoon Sohn","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhO0GLo0Ujp4N8ZrdKzJoyrNTSaaOokensF-GSqhQ=s64","userId":"12411594271774525837"}},"colab":{"base_uri":"https://localhost:8080/","height":190}},"source":["import tensorflow as tf\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pickle\n","import re\n","import nltk\n","import datetime\n","import os\n","from nltk.corpus import stopwords\n","from nltk.stem.snowball import SnowballStemmer\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","stemmer = SnowballStemmer(\"english\")\n","stops = set(stopwords.words(\"english\"))\n","\n","# from google.colab import files\n","# uploaded = files.upload()\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XPQkcPyT7Rl_","colab_type":"code","colab":{}},"source":["basedir = '/content/drive/My Drive/Colab Notebooks/Numeraxial/'\n","os.chdir(basedir)\n","filename = 'LM_dict.csv' # LM dictionary\n","# landm = pd.read_csv(os.path.join(basedir, filename),header=0, index_col='Word')\n","landm = pd.read_csv(filename,header=0, index_col='Word')\n","\n","filename = 'affectivespace.csv'\n","aff = pd.read_csv(filename,header=None, index_col=0)\n","aff.columns = pd.Index(np.arange(0,100))\n","aff.index.name = 'Word'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kvA4va0_7Sus","colab_type":"code","outputId":"24397bcc-bad3-4291-9bdc-1ebf3494090b","executionInfo":{"status":"ok","timestamp":1591288333702,"user_tz":240,"elapsed":428,"user":{"displayName":"Jongyoon Sohn","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhO0GLo0Ujp4N8ZrdKzJoyrNTSaaOokensF-GSqhQ=s64","userId":"12411594271774525837"}},"colab":{"base_uri":"https://localhost:8080/","height":105}},"source":["# Get the path of current working directory \n","path = os.getcwd() \n","  \n","# Get the list of all files and directories \n","# in current working directory \n","dir_list = os.listdir(path) \n","  \n","print(\"Files and directories in '\", path, \"' :\")  \n","# print the list \n","print(dir_list)\n","\n","idx_pickle=[]\n","idx_csv=[]\n","list_all = os.listdir(os.path.join(path,'data'))\n","for num, lst in enumerate(list_all):\n","  if re.findall(r\".pickle\", lst) != []:\n","    idx_pickle.append(num)\n","  elif re.findall(r\".csv\", lst) != []:\n","    idx_csv.append(num)\n","\n","pickle_set = []\n","csv_set = []\n","for i in idx_pickle:\n","  pickle_set.append(list_all[i])\n","for i in idx_csv:\n","  csv_set.append(list_all[i])\n","\n","pickle_set.sort()\n","csv_set.sort()\n","print(pickle_set)\n","print(csv_set)\n","n = len(pickle_set)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Files and directories in ' /content/drive/My Drive/Colab Notebooks/Numeraxial ' :\n","['affectivespace.csv', 'data', 'LM_dict.csv', 'NASDAQ.ipynb', 'Data_gen.ipynb', 'data_cleaned']\n","['news_AAPL_dict.pickle', 'news_AXP_dict.pickle', 'news_BA_dict.pickle', 'news_CAT_dict.pickle', 'news_CSCO_dict.pickle', 'news_CVX_dict.pickle', 'news_DIS_dict.pickle', 'news_GE_dict.pickle', 'news_GS_dict.pickle', 'news_HD_dict.pickle', 'news_IBM_dict.pickle', 'news_INTC_dict.pickle', 'news_JNJ_dict.pickle', 'news_JPM_dict.pickle', 'news_KO_dict.pickle', 'news_MCD_dict.pickle', 'news_MMM_dict.pickle', 'news_MRK_dict.pickle', 'news_MSFT_dict.pickle', 'news_NKE_dict.pickle', 'news_PFE_dict.pickle', 'news_PG_dict.pickle', 'news_TRV_dict.pickle', 'news_UNH_dict.pickle', 'news_UTX_dict.pickle', 'news_VZ_dict.pickle', 'news_V_dict.pickle', 'news_WMT_dict.pickle', 'news_XOM_dict.pickle']\n","['AAPL_20170102_20200529.csv', 'AXP_20170102_20200529.csv', 'BA_20170102_20200529.csv', 'CAT_20170102_20200529.csv', 'CSCO_20170102_20200529.csv', 'CVX_20170102_20200529.csv', 'DIS_20170102_20200529.csv', 'GE_20170102_20200529.csv', 'GS_20170102_20200529.csv', 'HD_20170102_20200529.csv', 'IBM_20170102_20200529.csv', 'INTC_20170102_20200529.csv', 'JNJ_20170102_20200529.csv', 'JPM_20170102_20200529.csv', 'KO_20170102_20200529.csv', 'MCD_20170102_20200529.csv', 'MMM_20170102_20200529.csv', 'MRK_20170102_20200529.csv', 'MSFT_20170102_20200529.csv', 'NKE_20170102_20200529.csv', 'PFE_20170102_20200529.csv', 'PG_20170102_20200529.csv', 'TRV_20170102_20200529.csv', 'UNH_20170102_20200529.csv', 'UTX_20170102_20200529.csv', 'VZ_20170102_20200529.csv', 'V_20170102_20200529.csv', 'WMT_20170102_20200529.csv', 'XOM_20170102_20200529.csv']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"94aR3saX7l6T","colab_type":"code","colab":{}},"source":["def create_dictionary(clean_word_list):\n","    word_count = {}\n","    for word in clean_word_list:\n","        if word in word_count:\n","            word_count[word] += 1\n","        else:\n","            word_count[word] = 1\n","    return word_count\n","\n","def findlist(your_list):\n","  l = [ind for ind in your_list if ind in aff.index]\n","  return l"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gT0UiYpL7aZm","colab_type":"code","outputId":"2d439ac6-0d9d-424e-ddd1-0712ba48c565","executionInfo":{"status":"ok","timestamp":1591288443542,"user_tz":240,"elapsed":38719,"user":{"displayName":"Jongyoon Sohn","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhO0GLo0Ujp4N8ZrdKzJoyrNTSaaOokensF-GSqhQ=s64","userId":"12411594271774525837"}},"colab":{"base_uri":"https://localhost:8080/","height":119}},"source":["for lp in range(n):\n","  \n","  filename_pkl = pickle_set[lp]\n","  with open(os.path.join('data', filename_pkl),'rb') as handle:\n","    b=pickle.load(handle)\n","\n","  tkr=[]\n","  pub_date=[]\n","  content=[]\n","\n","  for k, v in b.items():\n","    tkr.append(v['TICKER'])\n","    pub_date.append(v['PUBLICATION_DATE'])\n","    content.append(v['SUMMARY'])\n","\n","  df_pickle=pd.DataFrame({'Ticker':tkr,'Date':pub_date,'Summary':content})\n","  df_pickle['Date'] = pd.to_datetime(df_pickle['Date'], format = '%Y-%m-%d %H:%M:%S%z').dt.tz_convert('US/Eastern')\n","\n","\n","  fullnews=[]\n","  stemmer = SnowballStemmer(\"english\")\n","  stops = set(stopwords.words(\"english\"))\n","\n","  for review in df_pickle['Summary']:\n","    # review = val['SUMMARY']\n","    tokens = nltk.word_tokenize(str(review).lower())\n","    token_words = [w for w in tokens if w.isalpha()]\n","    # stemmed_words = [stemmer.stem(w) for w in token_words]\n","    meaningful_words = [w for w in token_words if not w in stops]\n","    fullnews.append(meaningful_words)\n","\n","  embedded = []\n","  for i in range(len(fullnews)):\n","    dict_words = create_dictionary(fullnews[i])\n","    wordsinAS = findlist(fullnews[i])\n","    \n","    final={}\n","    for _, key in enumerate(wordsinAS):\n","      final[key] = dict_words[key]\n","      \n","    summingup = 0.0\n","    for k, v in final.items():\n","      value = aff.loc[k] * v\n","      summingup += value.values\n","      \n","    if type(summingup) == float:\n","      embedded.append(np.zeros(100))\n","    else:\n","      embedded.append(summingup)#.tolist())\n","  \n","  df_final = pd.DataFrame(embedded)\n","  df_final['TICKER'] = b[0]['TICKER']\n","\n","  print('Stemming and tokenizing is complete!')\n","\n","  df_pickle['Date'] = pd.to_datetime(df_pickle['Date'], format = '%Y-%m-%d %H:%M:%S%z').dt.tz_convert('US/Eastern')\n","  df_final = pd.concat([df_pickle, df_final], axis=1)\n","  df_final.drop(['TICKER'], axis=1, inplace=True)\n","  df_final = df_final.set_index('Date').resample('15min').mean().dropna(axis = 'index', how = 'all').reset_index()\n","  df_final['Ticker'] = df_pickle['Ticker'][0]\n","  # Reoder columns\n","  cols = df_final.columns.to_list()[-1:] + df_final.columns.to_list()[:-1]\n","  df_final = df_final[cols]\n","\n","\n","  print('df_final is complete!')\n","  \n","  mkt_time = (df_final['Date'].dt.time >= datetime.time(9,30,00)) & (df_final['Date'].dt.time <= datetime.time(16,00,00))\n","  df_final['MktT'] = 'Close'\n","  df_final.loc[mkt_time, 'MktT'] = 'Open'\n","  tk = df_final['Ticker'][0]\n","  opendt = []\n","  embedded = []\n","  void_arr = []\n","  for y in range(df_final.shape[0]):\n","    if df_final.loc[y,'MktT'] == 'Close':\n","      void_arr.append(df_final.loc[y, np.arange(0,100)])\n","    else:\n","      opendt.append(df_final.loc[y, 'Date'])\n","      void_arr.append(df_final.loc[y, np.arange(0,100)])\n","      embedded.append(np.mean(void_arr, axis = 0))\n","      void_arr = []\n","\n","  new_df = pd.DataFrame({'Ticker': tk, 'Date': opendt})\n","  new_df = pd.concat([new_df, pd.DataFrame(embedded)], axis=1)\n","\n","\n","  print('new_df is complete!')\n","\n","  filename_csv = csv_set[lp]\n","  price_df = pd.read_csv(os.path.join('data', filename_csv), usecols=range(7))\n","  price_df['Date'] = pd.to_datetime(price_df['Date'], format='%d-%b-%Y %H:%M').dt.tz_localize('US/Eastern')\n","  new_df = new_df.drop(['Ticker'], axis=1)\n","  new_df = new_df.set_index(['Date'])\n","  price_df = price_df.set_index(['Date'])\n","  times = []\n","  for ix in price_df.index:\n","    if ix in new_df.index:\n","      times.append(ix)\n","  full_df = price_df.join(new_df).loc[times[0]:times[-1]].fillna(method='ffill')\n","\n","  print('full_df is complete!')\n","  full_df.to_pickle(tk + '.pkl')\n","  print('Uploaded '+ tk + ' data!')\n","  print('==================================================')\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Stemming and tokenizing is complete!\n","df_final is complete!\n","new_df is complete!\n","full_df is complete!\n","Uploaded DIS data!\n","==================================================\n"],"name":"stdout"}]}]}